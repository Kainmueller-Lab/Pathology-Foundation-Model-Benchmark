project: patho_benchmark
experiment: unetr
experiment_path: experiments
training_steps: 20
log_interval: 10
val_interval: 10
multiprocessing: false
num_workers: 0
seed: 42
primary_metric: f1_score_macro
save_all_ckpts: false # watch out for memory

dataset:
  name: 'lizard_small'
  path: tests/test_data/small_lizard_lmdb
  split: tests/test_data/small_lizard_train_test_val_split.csv
  label_dict: tests/test_data/label_dict.json
  batch_size: 2
  uniform_class_sampling: True
  sample_excluded_classes: False
  
model:
  model_wrapper: UnetR
  backbone: MOCK
  unfreeze_backbone: false

optimizer:
  name: AdamW
  params:
    lr: 3e-5
    weight_decay: 0.03
  warmup_steps: 100

scheduler:
  name: CosineAnnealingLR
  params:
    T_max: 200000
    eta_min: 1e-6

loss_fn:
  name: CrossEntropyLoss
  params:
    label_smoothing: 0.05
    reduction: none
  class_weighting: false

  exclude_classes: [0]

augmentations:
  RandomHorizontalFlip:
    p: 0.5
  RandomVerticalFlip:
    p: 0.5
  RandomAffine:
    degrees: [0, 360]
    translate: 0.1
    scale: [0.8, 1.2]  # Changed 0.2 to [min_scale, max_scale]
    resample: BILINEAR
  HEDNormalize:
    sigma: 0.025 # halved color aug
    bias: 0.05 # halved color aug
